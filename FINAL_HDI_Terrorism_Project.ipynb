{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73009266",
   "metadata": {},
   "source": [
    "# Term Project — HDI vs Terrorism (Frequency & Severity)\n",
    "\n",
    "**Goal:** test whether **Human Development Index (HDI)** is associated with  \n",
    "1) **terrorism frequency** (how many attacks) and  \n",
    "2) **terrorism severity** (how many casualties)\n",
    "\n",
    "using **interpretable ML models** (Linear Regression, Decision Tree, Random Forest) with a **time-aware evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "File: `dsa210project.xlsx` (already cleaned + engineered)\n",
    "\n",
    "Columns:\n",
    "- `country`, `region`, `year`\n",
    "- `hdi`\n",
    "- `attacks_count`, `total_casualty`\n",
    "- engineered:\n",
    "  - `log_attacks = log(attacks_count + 1)`\n",
    "  - `log_casualties = log(total_casualty + 1)`\n",
    "  - `hdi_group` (Low / Medium / High / Very High)\n",
    "  - `casualties_per_attack`, `log_casualties_per_attack`\n",
    "\n",
    "> We model **log** targets because terrorism data is heavy‑tailed (few extreme years/countries dominate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0) Imports + load data =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_PATH = r\"/mnt/data/dsa210project.xlsx\"\n",
    "df = pd.read_excel(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick checks\n",
    "display(df.isna().sum())\n",
    "display(df.dtypes)\n",
    "\n",
    "# Enforce numeric types (defensive)\n",
    "num_cols = [\"year\",\"hdi\",\"attacks_count\",\"total_casualty\",\n",
    "            \"log_attacks\",\"log_casualties\",\"casualties_per_attack\",\"log_casualties_per_attack\"]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Drop rows missing critical info\n",
    "df = df.dropna(subset=[\"country\",\"region\",\"year\",\"hdi\",\"attacks_count\",\"total_casualty\"]).copy()\n",
    "\n",
    "# Replace infinities if any\n",
    "for c in [\"casualties_per_attack\",\"log_casualties_per_attack\"]:\n",
    "    df[c] = df[c].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(\"After cleaning:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892668ef",
   "metadata": {},
   "source": [
    "## 1) Exploratory Data Analysis (EDA)\n",
    "\n",
    "We will check:\n",
    "- distributions (are there extreme tails?)\n",
    "- HDI vs outcomes (scatter)\n",
    "- trend over time (global + region)\n",
    "- simple correlations (HDI with log targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Distributions\n",
    "targets = [\"attacks_count\",\"total_casualty\",\"log_attacks\",\"log_casualties\",\"log_casualties_per_attack\"]\n",
    "\n",
    "for t in targets:\n",
    "    plt.figure()\n",
    "    plt.hist(df[t].dropna(), bins=50)\n",
    "    plt.xlabel(t)\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.title(f\"Distribution: {t}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Scatter: HDI vs log targets\n",
    "pairs = [(\"hdi\",\"log_attacks\"), (\"hdi\",\"log_casualties\"), (\"hdi\",\"log_casualties_per_attack\")]\n",
    "\n",
    "for x,y in pairs:\n",
    "    plt.figure()\n",
    "    plt.scatter(df[x], df[y], alpha=0.4)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.title(f\"{x} vs {y}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Time trends: global median by year (robust to outliers)\n",
    "yearly = (df.groupby(\"year\", as_index=False)\n",
    "            .agg(hdi=(\"hdi\",\"median\"),\n",
    "                 log_attacks=(\"log_attacks\",\"median\"),\n",
    "                 log_casualties=(\"log_casualties\",\"median\")))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(yearly[\"year\"], yearly[\"hdi\"])\n",
    "plt.xlabel(\"year\"); plt.ylabel(\"median HDI\")\n",
    "plt.title(\"Global median HDI over time\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(yearly[\"year\"], yearly[\"log_attacks\"])\n",
    "plt.xlabel(\"year\"); plt.ylabel(\"median log_attacks\")\n",
    "plt.title(\"Global median log_attacks over time\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(yearly[\"year\"], yearly[\"log_casualties\"])\n",
    "plt.xlabel(\"year\"); plt.ylabel(\"median log_casualties\")\n",
    "plt.title(\"Global median log_casualties over time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00122904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Region-year view (median)\n",
    "region_year = (df.groupby([\"region\",\"year\"], as_index=False)\n",
    "                 .agg(hdi=(\"hdi\",\"median\"),\n",
    "                      log_attacks=(\"log_attacks\",\"median\"),\n",
    "                      log_casualties=(\"log_casualties\",\"median\")))\n",
    "\n",
    "plt.figure()\n",
    "for r in sorted(region_year[\"region\"].unique()):\n",
    "    sub = region_year[region_year[\"region\"]==r]\n",
    "    plt.scatter(sub[\"hdi\"], sub[\"log_attacks\"], alpha=0.6, label=r)\n",
    "plt.xlabel(\"median HDI (region-year)\")\n",
    "plt.ylabel(\"median log_attacks\")\n",
    "plt.title(\"Region-year: HDI vs log_attacks\")\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Correlation (simple, not causal)\n",
    "corr = df[[\"hdi\",\"log_attacks\",\"log_casualties\",\"log_casualties_per_attack\"]].corr(numeric_only=True)\n",
    "display(corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf34fc0",
   "metadata": {},
   "source": [
    "## 2) Modeling setup (time-aware split)\n",
    "\n",
    "Because the data is yearly, we use **time-based split**:\n",
    "- Train = earlier years\n",
    "- Test  = later years\n",
    "\n",
    "This helps avoid information leakage (future → past).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose cutoff year (edit if you want)\n",
    "cutoff_year = 2012\n",
    "\n",
    "# Feature set\n",
    "feature_cols = [\"hdi\", \"year\", \"country\", \"region\", \"hdi_group\"]\n",
    "\n",
    "# Targets we will model\n",
    "TARGETS = [\"log_attacks\", \"log_casualties\", \"log_casualties_per_attack\"]\n",
    "\n",
    "# Prepare split once\n",
    "data_all = df[feature_cols + TARGETS].dropna().copy()\n",
    "train = data_all[data_all[\"year\"] <= cutoff_year].copy()\n",
    "test  = data_all[data_all[\"year\"] >  cutoff_year].copy()\n",
    "\n",
    "print(\"Train:\", train.shape, \"years\", train[\"year\"].min(), \"-\", train[\"year\"].max())\n",
    "print(\"Test :\", test.shape,  \"years\", test[\"year\"].min(),  \"-\", test[\"year\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c461d573",
   "metadata": {},
   "source": [
    "## 3) Pipelines + evaluation metrics\n",
    "\n",
    "We build a single preprocessing transformer:\n",
    "- numeric: `hdi`, `year` (scaled)\n",
    "- categorical: `country`, `region`, `hdi_group` (one-hot)\n",
    "\n",
    "Then we fit 3 models:\n",
    "- Linear Regression (baseline)\n",
    "- Decision Tree (tuned)\n",
    "- Random Forest (tuned)\n",
    "\n",
    "Metrics:\n",
    "- **MAE**\n",
    "- **RMSE**\n",
    "- **R²**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cat_cols = [\"country\",\"region\",\"hdi_group\"]\n",
    "num_cols = [\"hdi\",\"year\"]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def eval_reg(y_true, y_pred):\n",
    "    return {\n",
    "        \"MAE\":  mean_absolute_error(y_true, y_pred),\n",
    "        \"RMSE\": mean_squared_error(y_true, y_pred, squared=False),\n",
    "        \"R2\":   r2_score(y_true, y_pred),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8142d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling function (returns fitted models + results row)\n",
    "def fit_models_for_target(target, X_train, y_train, X_test, y_test):\n",
    "    rows = []\n",
    "    fitted = {}\n",
    "\n",
    "    # 1) Linear Regression\n",
    "    lin = Pipeline([(\"prep\", preprocess), (\"model\", LinearRegression())])\n",
    "    lin.fit(X_train, y_train)\n",
    "    pred = lin.predict(X_test)\n",
    "    rows.append({\"target\": target, \"model\": \"LinearRegression\", **eval_reg(y_test, pred)})\n",
    "    fitted[\"LinearRegression\"] = lin\n",
    "\n",
    "    # 2) Decision Tree (tuned - small grid to keep runtime reasonable)\n",
    "    tree = Pipeline([(\"prep\", preprocess), (\"model\", DecisionTreeRegressor(random_state=42))])\n",
    "    grid_tree = {\n",
    "        \"model__max_depth\": [3, 5, 8, None],\n",
    "        \"model__min_samples_leaf\": [1, 2, 5, 10],\n",
    "        \"model__min_samples_split\": [2, 5, 10],\n",
    "    }\n",
    "    gs_tree = GridSearchCV(tree, grid_tree, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "    gs_tree.fit(X_train, y_train)\n",
    "    best_tree = gs_tree.best_estimator_\n",
    "    pred = best_tree.predict(X_test)\n",
    "    rows.append({\"target\": target, \"model\": f\"DecisionTree(tuned)\", **eval_reg(y_test, pred),\n",
    "                 \"best_params\": gs_tree.best_params_})\n",
    "    fitted[\"DecisionTree\"] = best_tree\n",
    "\n",
    "    # 3) Random Forest (tuned - small grid)\n",
    "    rf = Pipeline([(\"prep\", preprocess), (\"model\", RandomForestRegressor(random_state=42, n_jobs=-1))])\n",
    "    grid_rf = {\n",
    "        \"model__n_estimators\": [200, 400],\n",
    "        \"model__max_depth\": [None, 10, 20],\n",
    "        \"model__min_samples_leaf\": [1, 2, 5],\n",
    "    }\n",
    "    gs_rf = GridSearchCV(rf, grid_rf, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "    gs_rf.fit(X_train, y_train)\n",
    "    best_rf = gs_rf.best_estimator_\n",
    "    pred = best_rf.predict(X_test)\n",
    "    rows.append({\"target\": target, \"model\": f\"RandomForest(tuned)\", **eval_reg(y_test, pred),\n",
    "                 \"best_params\": gs_rf.best_params_})\n",
    "    fitted[\"RandomForest\"] = best_rf\n",
    "\n",
    "    return rows, fitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5657f",
   "metadata": {},
   "source": [
    "## 4) Run models for all targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rows = []\n",
    "all_fitted = {}\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "X_test  = test[feature_cols]\n",
    "\n",
    "for target in TARGETS:\n",
    "    y_train = train[target]\n",
    "    y_test  = test[target]\n",
    "    rows, fitted = fit_models_for_target(target, X_train, y_train, X_test, y_test)\n",
    "    results_rows.extend(rows)\n",
    "    all_fitted[target] = fitted\n",
    "\n",
    "results = pd.DataFrame(results_rows)\n",
    "display(results.sort_values([\"target\",\"RMSE\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d78bf",
   "metadata": {},
   "source": [
    "## 5) Diagnostics: residuals vs HDI (for the best model per target)\n",
    "\n",
    "If residuals show patterns across HDI, it suggests the model still misses structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best model by lowest RMSE per target\n",
    "best_by_target = (results.sort_values(\"RMSE\")\n",
    "                  .groupby(\"target\", as_index=False)\n",
    "                  .first()[[\"target\",\"model\"]])\n",
    "display(best_by_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_obj(target, model_name):\n",
    "    # normalize labels\n",
    "    if \"Linear\" in model_name:\n",
    "        return all_fitted[target][\"LinearRegression\"]\n",
    "    if \"DecisionTree\" in model_name:\n",
    "        return all_fitted[target][\"DecisionTree\"]\n",
    "    return all_fitted[target][\"RandomForest\"]\n",
    "\n",
    "for _, row in best_by_target.iterrows():\n",
    "    target = row[\"target\"]\n",
    "    model_name = row[\"model\"]\n",
    "    model = get_model_obj(target, model_name)\n",
    "    y_test = test[target]\n",
    "    pred = model.predict(X_test)\n",
    "    residuals = y_test.values - pred\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(X_test[\"hdi\"], residuals, alpha=0.5)\n",
    "    plt.axhline(0)\n",
    "    plt.xlabel(\"HDI\")\n",
    "    plt.ylabel(\"Residual (true - pred)\")\n",
    "    plt.title(f\"Residuals vs HDI | target={target} | model={model_name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9169feb1",
   "metadata": {},
   "source": [
    "## 6) Feature importance (Random Forest)\n",
    "\n",
    "One-hot encoding creates many features (country/region dummies).\n",
    "We print **top 20** important features for each target's random forest model.\n",
    "\n",
    "> If `hdi` is consistently among top features, that's evidence HDI contributes predictive signal **after controlling for** region/country/year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad29f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rf_feature_importance_table(rf_pipeline, top_n=20):\n",
    "    ohe = rf_pipeline.named_steps[\"prep\"].named_transformers_[\"cat\"]\n",
    "    cat_names = ohe.get_feature_names_out(cat_cols)\n",
    "    feature_names = np.concatenate([num_cols, cat_names])\n",
    "    importances = rf_pipeline.named_steps[\"model\"].feature_importances_\n",
    "    fi = (pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "            .sort_values(\"importance\", ascending=False)\n",
    "            .head(top_n))\n",
    "    return fi\n",
    "\n",
    "for target in TARGETS:\n",
    "    rf_model = all_fitted[target][\"RandomForest\"]\n",
    "    print(\"\\n=== Target:\", target, \"===\")\n",
    "    display(rf_feature_importance_table(rf_model, top_n=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77726ea",
   "metadata": {},
   "source": [
    "## 7) (Optional) “HDI effect” with a simple what-if curve\n",
    "\n",
    "We hold `country/region/hdi_group` fixed and vary HDI to see the model’s predicted trend.\n",
    "\n",
    "This is **not causal**, but it helps your discussion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_if_hdi_curve(target, country, region, hdi_group, year, model_kind=\"RandomForest\"):\n",
    "    model = all_fitted[target][model_kind]\n",
    "    hdi_grid = np.linspace(df[\"hdi\"].min(), df[\"hdi\"].max(), 60)\n",
    "\n",
    "    Xw = pd.DataFrame({\n",
    "        \"hdi\": hdi_grid,\n",
    "        \"year\": [year]*len(hdi_grid),\n",
    "        \"country\": [country]*len(hdi_grid),\n",
    "        \"region\": [region]*len(hdi_grid),\n",
    "        \"hdi_group\": [hdi_group]*len(hdi_grid),\n",
    "    })\n",
    "\n",
    "    pred = model.predict(Xw)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hdi_grid, pred)\n",
    "    plt.xlabel(\"HDI\")\n",
    "    plt.ylabel(f\"Predicted {target}\")\n",
    "    plt.title(f\"What-if: {target} vs HDI | {model_kind} | {country}, {year}\")\n",
    "    plt.show()\n",
    "\n",
    "# Example (edit freely)\n",
    "what_if_hdi_curve(\n",
    "    target=\"log_attacks\",\n",
    "    country=\"Turkey\",\n",
    "    region=\"Middle East & North Africa\",\n",
    "    hdi_group=\"High\",\n",
    "    year=2015,\n",
    "    model_kind=\"RandomForest\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b4935",
   "metadata": {},
   "source": [
    "## 8) Save results to Excel/CSV (for your report)\n",
    "\n",
    "You can paste tables + screenshots of plots to your report, but it's nice to also export the metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5787ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv = \"model_results.csv\"\n",
    "results.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900a69d",
   "metadata": {},
   "source": [
    "# Report checklist (copy into your write-up)\n",
    "\n",
    "- **Research question:** Is HDI associated with terrorism frequency/severity?\n",
    "- **Targets:** log_attacks, log_casualties, log_casualties_per_attack\n",
    "- **Why log transform:** heavy tails/outliers\n",
    "- **Train/test split:** time-based (train <= cutoff_year, test > cutoff_year)\n",
    "- **Models:** Linear Regression, tuned Decision Tree, tuned Random Forest\n",
    "- **Metrics:** MAE/RMSE/R² (compare models)\n",
    "- **Interpretation:**\n",
    "  - Feature importance (is HDI important after controls?)\n",
    "  - Residual vs HDI plot (systematic patterns?)\n",
    "  - Optional what-if HDI curve (discussion aid)\n",
    "- **Limitations:** correlation vs causality; missing confounders; measurement errors; aggregation by country-year\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
